# Data-Science-Projects
This repository is a collection of my data science projects, where I apply various analysis and visualization techniques.
### Exercise1_UsingDifferentPlots
This repository contains various data visualization examples using different types of charts, each suited for specific datasets and insights. The visualizations cover a wide range of data, including sales trends, weather conditions, sports performance, and market analysis. Each section includes a brief explanation of why a particular visualization was chosen and what insights it provides.

These examples can serve as a reference for selecting appropriate visualization techniques based on dataset characteristics and analysis goals.


### Exercise2_MNIST-KNN-Feature-Extraction
The set of scripts processes the MNIST dataset, first converting the raw images into flattened vectors with labels and saving them for use in machine learning. The second script performs K-Nearest Neighbors (K-NN) classification with 10-fold cross-validation on this preprocessed data, evaluating model performance. The third script enhances the dataset by extracting additional features, such as average pixel value, edge count, and aspect ratio, before performing the same cross-validation classification process. The final script evaluates the impact of excluding individual features on classification accuracy, helping assess the importance of each feature. Together, these scripts demonstrate a comprehensive workflow of data preparation, feature extraction, and model evaluation.


### Exercise3_MNIST- MultiDataset-ML-Evaluation
The provided code implements machine learning classification and clustering techniques on three datasets: Bank, Wine, and Bean. It preprocesses categorical features using label encoding and evaluates models using multiple validation methods, including train-test split, Leave-One-Out (LOO), and 10-fold cross-validation. The classifiers used include Na√Øve Bayes, K-Nearest Neighbors (KNN), Random Forest, and Decision Tree (Rep Tree equivalent), with KNN being tuned for optimal performance. Additionally, K-Means and a variant with limited iterations (Farthest First Clustering) are used for unsupervised learning. The results, including accuracy scores, confusion matrices, and classification reports, are saved in separate text files for analysis.